)
solve((t(x) %*% x
)
)
pmax(0, sigma_2 * solve((t(x) %*% x)))))
pmax(0, sigma_2 * solve((t(x) %*% x))))
pmax(0, sigma_2 * solve((t(x) %*% x)))
solve((t(x) %*% x ))
sigma_2 * solve((t(x) %*% x ))
my_solve <- sigma_2 * solve((t(x) %*% x))
my_solve
for (i in nrow(my_solve)) {
for(j in ncol(my_solve))
pmax(0, my_solve[i, j])
}
my_solve
for (i in nrow(my_solve)) {
+     for(j in ncol(my_solve))
+         pmax(0, my_solve[[i, j]])
+ }
for (i in nrow(my_solve)) {
for(j in ncol(my_solve)){
if(my_solve[i, j ] < 0)
my_solve[i,j] = 0
}
}
my_solve
for (i in 1:nrow(my_solve)) {
for(j in 1:ncol(my_solve)){
if(my_solve[i, j ] < 0)
my_solve[i,j] = 0
}
}
my_solve
std_error <- diag(sqrt(my_solve))
std_error
knitr::opts_chunk$set(echo = TRUE)
#loads mtcars data set
data(mtcars)
test_data = mtcars
test_formula = mtcars$mpg ~ mtcars$hp
#creates my_lm function
my_lm <- function(formula, data){
#extracts model frame
model_frame <- model.frame(formula, data)
#extracts model matrix x
x <- model.matrix(formula, data = data)
#extracts model response y
y <- model.response(model_frame)
#finds coefficients using given formula
beta <- solve(t(x) %*% x) %*% t(x) %*% y
#calculates degrees of freedom
df <- nrow(data) - ncol(model_frame)
#calculates sigma^2
sigma_2 <- sum(((y - x %*% beta) ^ 2) / df)
#calculates standard error, after setting negative values to zero
my_solve <- sigma_2 * solve((t(x) %*% x))
for (i in 1:nrow(my_solve)) {
for(j in 1:ncol(my_solve)){
if(my_solve[i, j ] < 0)
my_solve[i,j] = 0
}
}
std_error <- diag(sqrt(my_solve))
#calculates t values
t_val <- beta / std_error
#Calculates Pr(>|t|)
Pr_less_than_abs_t <- 2 * pt(abs(t_val), df, lower.tail = FALSE)
#creates data frame for table
output <- data.frame("Estimate" = c(beta),
"Std. Error" = c(std_error),
"t value" = c(t_val),
"Pr(>|t|)" = c(Pr_less_than_abs_t)
)
#properly labels rownames
rownames(output) <- rownames(beta)
#creates table
library(knitr)
library(kableExtra)
return(kable_styling(kable(output)))
}
#tests my_lm by comparing to lm()
my_lm(test_formula, test_data)
summary(lm(test_formula, test_data))
lm(formula, data)
my_lm(formula, data)
summary(lm(formula, data))
devtools::document()
library(STAT302Package)
devtools::document()
rm(list = c("data", "my_lm"))
devtools::document()
devtools::document()
devtools::check()
?kable
devtools::check()
devtools::check()
devtools::check()
devtools::check()
my_lm(lifeExp ~ gdpPercap, my_gapminder)
devtools::document()
devtools::check()
testthat::expect_equal(df, nrow(data) - ncol(model_frame))
testthat::expect_gt(my_lm("lifeExp ~ gdpPercap", my_gapminder)$df, 0)
testthat::expect_gt(df, 0)
testthat::expect_is(my_lm("lifeExp ~ gdpPercap", "table")
)
testthat::expect_is(my_lm("lifeExp ~ gdpPercap", my_gapminder), "table")
testthat::expect_is(my_lm("lifeExp ~ gdpPercap", my_gapminder), "table")
devtools::check()
sample_output <- my_lm("lifeExp ~ gdpPercap", my_gapminder)
sample_output <- my_lm("lifeExp ~ gdpPercap", my_gapminder)
library(STAT302Package)
sample_output <- my_lm("lifeExp ~ gdpPercap", my_gapminder)
my_lm("lifeExp ~ gdpPercap", my_gapminder)
my_lm("lifeExp ~ gdpPercap", my_gapminder)
devtools::check()
devtools::check()
my_lm(lifeExp ~ gdpPercap, my_gapminder)
ncol(my_lm(lifeExp ~ gdpPercap, my_gapminder))
devtools::document()
devtools::check()
devtools::document()
devtools::check()
my_rf_cv(5)
testthat::expect_is(my_lm(lifeExp ~ gdpPercap, "my_gapminder"), "kable")
testthat::expect_is(my_lm(lifeExp ~ gdpPercap, "my_gapminder"), "list")
testthat::expect_is(my_lm(lifeExp ~ gdpPercap, my_gapminder), "kable")
testthat::expect_is(my_lm(lifeExp ~ gdpPercap, my_gapminder), "kableExtra/knitr_kable")
testthat::expect_type(my_lm(lifeExp ~ gdpPercap, my_gapminder), "kable")
testthat::expect_is(my_lm(lifeExp ~ gdpPercap, my_gapminder), "character")
estthat::expect_type(my_lm(lifeExp ~ gdpPercap, my_gapminder), "character")
5estthat::expect_type(my_lm(lifeExp ~ gdpPercap, my_gapminder), "character")
testthat::expect_type(my_lm(lifeExp ~ gdpPercap, my_gapminder), "character")
class(my_lm(lifeExp ~ gdpPercap, my_gapminder))
testthat::expect_type(my_lm(lifeExp ~ gdpPercap, my_gapminder), "knitr_kable")
devtools::document()
devtools::check()
my_lm(lifeExp ~ gdpPercap, my_gapminder)
my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
my_knn_cv(my_penguins, penguins$species, 1, 5)
my_knn_cv(na.omit(my_penguins), penguins$species, 1, 5)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(randomForest)
my_data <- train
library(palmerpenguins)
data(package = "palmerpenguins")
data(penguins)
library(class)
library(tidyverse)
#removes observations with na values
penguins <- na.omit(penguins)
#removes unnecessary columns from dataset
train <- penguins
train$island <- NULL
train$sex <- NULL
train$year <- NULL
#creates function
my_knn_cv <- function(train, cl, k_nn, k_cv){
#randomly assigns observations to folds
train$fold <- sample(rep(1:k_cv, length = nrow(train)))
fold <- train$fold
#creates column to store prediction for all observations
class <- rep(NA, nrow(train))
#creates vector to store error calculations
misclass_rate <- rep(NA, k_cv)
#uses for loop to repeat steps for each fold
for (i in 1:k_cv) {
#creates train and test data
test_data <- train %>% filter(fold == i)
train_data <- train %>% filter(fold != i)
#creates vector of classes for both data sets
cl_train <- train_data$species
cl_test <- test_data$species
#removes class and fold columns from datasets
train_data <- train_data[ ,3:ncol(train_data)-1]
test_data <- test_data[ ,3:ncol(test_data)-1]
#stores predictions in dataset
class[train$fold == i] <- knn(train_data,
test_data,
cl_train,
k = k_nn,
prob = TRUE)
#creates vector to store misclassifications
error <- rep(NA, nrow(test_data))
#labels misclassifications
for (j in 1:nrow(test_data)) {
error[train$fold == j] =
(as.numeric(train$species[train$fold == j])) != class[train$fold == j]
}
#calculates misclassification rate
misclass_rate[i] <- sum(na.omit(as.numeric(error[train$fold == i])))
misclass_rate[i] <- misclass_rate[i] / length(cl_test)
}
#calculates mean misclassification rate
cv_err <- mean(misclass_rate)
#creates list of objects to return
my_list <- list()
my_list$cv_err <- cv_err
my_list$class <- class
#returns list
return(my_list)
}
#tests function
knn_1 <- my_knn_cv(train, cl, 1, 5)
knn_5 <- my_knn_cv(train, cl, 5, 5)
training_err_1 <- mean((knn_1$class - as.numeric(penguins$species))^2)
training_err_5 <- mean((knn_5$class - as.numeric(penguins$species))^2)
my_data <- data.frame("k_nn" = c(1,5),
"Misclassification Error" = c(knn_1$cv_err, knn_5$cv_err),
"Training Error" = c(training_err_1, training_err_5))
library(knitr)
library(kableExtra)
kable_styling(kable(my_data))
library(tidyverse)
library(randomForest)
my_data <- train
my_rf_cv <- function(k){
#creates vector to store cross validation errors
cv_errors_2 <- rep(NA, k)
#gives each observation a fold
my_data$fold <- sample(rep(1:k, length = nrow(my_data)))
for (i in 1:k) {
#creates training data out of data not in ith fold
data_train <- my_data %>% filter(fold != i)
data_test <- my_data %>% filter(fold == i)
#creates model with randomForest()
my_model <- randomForest(
body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
data = data_train,
ntree = 100
)
#creates vector of predictions
predictions_2 <- predict(my_model, data_test[, -1])
cv_errors_2[i] <- mean((predictions_2 - data_test$body_mass_g)^2)
}
#returns mean standard error
return(mean(cv_errors_2))
}
cv_mse <- my_rf_cv(5)
library(class)
library(tidyverse)
#removes observations with na values
penguins <- na.omit(penguins)
#removes unnecessary columns from dataset
train <- penguins
train$island <- NULL
train$sex <- NULL
train$year <- NULL
#creates function
my_knn_cv <- function(train, cl, k_nn, k_cv){
#randomly assigns observations to folds
train$fold <- sample(rep(1:k_cv, length = nrow(train)))
fold <- train$fold
#creates column to store prediction for all observations
class <- rep(NA, nrow(train))
#creates vector to store error calculations
misclass_rate <- rep(NA, k_cv)
#uses for loop to repeat steps for each fold
for (i in 1:k_cv) {
#creates train and test data
test_data <- train %>% filter(fold == i)
train_data <- train %>% filter(fold != i)
#creates vector of classes for both data sets
cl_train <- train_data$species
cl_test <- test_data$species
#removes class and fold columns from datasets
train_data <- train_data[ ,3:ncol(train_data)-1]
test_data <- test_data[ ,3:ncol(test_data)-1]
#stores predictions in dataset
class[train$fold == i] <- knn(train_data,
test_data,
cl_train,
k = k_nn,
prob = TRUE)
#creates vector to store misclassifications
error <- rep(NA, nrow(test_data))
#labels misclassifications
for (j in 1:nrow(test_data)) {
error[train$fold == j] =
(as.numeric(train$species[train$fold == j])) != class[train$fold == j]
}
#calculates misclassification rate
misclass_rate[i] <- sum(na.omit(as.numeric(error[train$fold == i])))
misclass_rate[i] <- misclass_rate[i] / length(cl_test)
}
#calculates mean misclassification rate
cv_err <- mean(misclass_rate)
#creates list of objects to return
my_list <- list()
my_list$cv_err <- cv_err
my_list$class <- class
#returns list
return(my_list)
}
#tests function
knn_1 <- my_knn_cv(train, cl, 1, 5)
knn_5 <- my_knn_cv(train, cl, 5, 5)
training_err_1 <- mean((knn_1$class - as.numeric(penguins$species))^2)
training_err_5 <- mean((knn_5$class - as.numeric(penguins$species))^2)
my_data <- data.frame("k_nn" = c(1,5),
"Misclassification Error" = c(knn_1$cv_err, knn_5$cv_err),
"Training Error" = c(training_err_1, training_err_5))
library(knitr)
library(kableExtra)
kable_styling(kable(my_data))
summary(lm(lifeExp ~ gdpPercap + continent, my_gapminder))
formula = lifeExp ~ gdpPercap + continent
data <- my_gapminder
formula <- lifeExp ~ gdpPercap + continent
model_frame <- model.frame(formula, data)
x <- model.matrix(formula, data = data)
y <- model.response(model_frame)
beta <- solve(t(x) %*% x) %*% t(x) %*% y
df <- nrow(data) - ncol(model_frame)
sigma_2 <- sum(((y - x %*% beta) ^ 2) / df)
my_solve <- sigma_2 * solve((t(x) %*% x))
my_solve
my_vector <- my_gapminder$gdpPercap
my_vector
my_vector <- my_gapminder$continent
my_vector
library(STAT302Package)
data("my_gapminder")
data("my_penguins")
fitted(my_lm(lifeExp ~ gdpPercap + continent, my_gapminder))
ggplot(my_gapminder$lifeExp, aes(x = fitted, y = actual)) +
geom_point()
ggplot(my_gapminder, aes(x = fitted, y = actual)) +
geom_point()
lm(lifeExp ~ gdpPercap + continent, my_gapminder)
#Creates data frame of actual and fitted values
mod_fits <- fitted(my_lm)
#Creates data frame of actual and fitted values
mod_fits <- fitted(my_lm(lifeExp ~ gdpPercap + continent, my_gapminder))
#Creates data frame of actual and fitted values
mod_fits <- fitted(lm(lifeExp ~ gdpPercap + continent, my_gapminder))
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = mod_fits)
#Plots actual vs. fitted graph
ggplot(my_df, aes(x = fitted, y = actual)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) +
theme_bw(base_size = 15) +
labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
theme(plot.title = element_text(hjust = 0.5))
devtools::check()
devtools::document()
rm(list = c("data", "my_knn_cv", "my_rf_cv"))
devtools::document()
devtools::check()
devtools::check()
devtools::check()
my_rf_cv(5)
library(tidyverse)
library(randomForest)
my_data <- train
library(palmerpenguins)
data(package = "palmerpenguins")
data(penguins)
library(class)
library(tidyverse)
#removes observations with na values
penguins <- na.omit(penguins)
#removes unnecessary columns from dataset
train <- penguins
train$island <- NULL
train$sex <- NULL
train$year <- NULL
#creates function
my_knn_cv <- function(train, cl, k_nn, k_cv){
#randomly assigns observations to folds
train$fold <- sample(rep(1:k_cv, length = nrow(train)))
fold <- train$fold
#creates column to store prediction for all observations
class <- rep(NA, nrow(train))
#creates vector to store error calculations
misclass_rate <- rep(NA, k_cv)
#uses for loop to repeat steps for each fold
for (i in 1:k_cv) {
#creates train and test data
test_data <- train %>% filter(fold == i)
train_data <- train %>% filter(fold != i)
#creates vector of classes for both data sets
cl_train <- train_data$species
cl_test <- test_data$species
#removes class and fold columns from datasets
train_data <- train_data[ ,3:ncol(train_data)-1]
test_data <- test_data[ ,3:ncol(test_data)-1]
#stores predictions in dataset
class[train$fold == i] <- knn(train_data,
test_data,
cl_train,
k = k_nn,
prob = TRUE)
#creates vector to store misclassifications
error <- rep(NA, nrow(test_data))
#labels misclassifications
for (j in 1:nrow(test_data)) {
error[train$fold == j] =
(as.numeric(train$species[train$fold == j])) != class[train$fold == j]
}
#calculates misclassification rate
misclass_rate[i] <- sum(na.omit(as.numeric(error[train$fold == i])))
misclass_rate[i] <- misclass_rate[i] / length(cl_test)
}
#calculates mean misclassification rate
cv_err <- mean(misclass_rate)
#creates list of objects to return
my_list <- list()
my_list$cv_err <- cv_err
my_list$class <- class
#returns list
return(my_list)
}
#tests function
knn_1 <- my_knn_cv(train, cl, 1, 5)
knn_5 <- my_knn_cv(train, cl, 5, 5)
training_err_1 <- mean((knn_1$class - as.numeric(penguins$species))^2)
training_err_5 <- mean((knn_5$class - as.numeric(penguins$species))^2)
my_data <- data.frame("k_nn" = c(1,5),
"Misclassification Error" = c(knn_1$cv_err, knn_5$cv_err),
"Training Error" = c(training_err_1, training_err_5))
library(knitr)
library(kableExtra)
kable_styling(kable(my_data))
library(tidyverse)
library(randomForest)
my_data <- train
my_rf_cv <- function(k){
#creates vector to store cross validation errors
cv_errors_2 <- rep(NA, k)
#gives each observation a fold
my_data$fold <- sample(rep(1:k, length = nrow(my_data)))
for (i in 1:k) {
#creates training data out of data not in ith fold
data_train <- my_data %>% filter(fold != i)
data_test <- my_data %>% filter(fold == i)
#creates model with randomForest()
my_model <- randomForest(
body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
data = data_train,
ntree = 100
)
#creates vector of predictions
predictions_2 <- predict(my_model, data_test[, -1])
cv_errors_2[i] <- mean((predictions_2 - data_test$body_mass_g)^2)
}
#returns mean standard error
return(mean(cv_errors_2))
}
cv_mse <- my_rf_cv(5)
library(tidyverse)
library(randomForest)
my_data <- train
k <- 5
cv_errors_2 <- rep(NA, k)
my_data$fold <- sample(rep(1:k, length = nrow(my_data)))
View(my_data)
my_rf_cv
my_rf_cv(5)
my_rf_cv(5)
my_data <- na.omit(my_penguins)
my_rf_cv
my_rf_cv(5)
my_rf_cv()
my_rf_cv(5)
my_rf_cv(5)
my_rf_cv(5)
my_rf_cv(5)
my_data <- data(my_penguins, envir = environment())
my_data <- na.omit(my_data)
my_rf_cv(5)
my_data <- data("my_penguins", envir = environment())
my_data <- my_penguins
data(my_penguins, envir = environment())
data(my_penguins, envir = environment())
my_rf_cv(5)
my_data <- my_penguins
my_rf_cv(5)
my_rf_cv(5)
my_rf_cv(5)
library(STAT302Package)
my_rf_cv(5)
my_rf_cv(5)
my_data <- na.omit(my_penguins)
sample(rep(1:k, length = nrow(my_data)))
sample(rep(1:5, length = nrow(my_data)))
fold <- sample(rep(1:k, length = nrow(my_data)))
fold <- sample(rep(1:5, length = nrow(my_data)))
View(my_data)
fold <- as.data.frame(sample(rep(1:5, length = nrow(my_data))))
my_data$fold <- fold
View(my_data)
my_data <- na.omit(my_penguins)
fold <- as.data.frame(sample(rep(1:k, length = nrow(my_data))))
View(fold)
fold <- sample(rep(1:k, length = nrow(my_data)))
fold <- sample(rep(1:5, length = nrow(my_data)))
my_data$fold <- fold
View(my_data)
my_rf_cv(5)
my_data <- na.omit(my_penguins)
my_rf_cv(5)
my_data <- na.omit(my_penguins)
fold <- sample(rep(1:k, length = nrow(my_data)))
fold <- sample(rep(1:5, length = nrow(my_data)))
my_data <- cbind(my_data, fold)
View(my_data)
my_rf_cv(5)
library(STAT302Package)
my_rf_cv(5)
