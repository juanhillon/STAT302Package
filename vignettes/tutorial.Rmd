---
title: "Project 2: STAT302Package Tutorial"
author: "Juan Hillon"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302Package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(STAT302Package)
library(ggplot2)
library(kableExtra)
data("my_gapminder")
data("my_penguins")
```

## Introduction
This package demonstrates package building, using functions from previous labs. 
These functions are `my_t.test`, a t test function, `my_lm`, a linear model function, `my_knn_cv`, a k-nearest neighbors cross-validation function, and `my_rf_cv`, a rondom forest cross-validation function. I will be using the datasets `my_penguins` and `my_gapminder` to demonstrate these functions.

To install and load STAT302Package, use the following code:
```{r, eval=FALSE}
devtools::install_github("juanhillon/STAT302Package")
library(STAT302Package)
```

## Tutorial for `my_t.test`
To explain how to use `my_t.test`, I will be demonstrating three t-tests with a p-value cut-off of 0.05 using the `lifeExp` data from `my_gapminder` and explaining the results. This function returns a list with four elements. The first item represents the test statistic, which is equal to `(mean(x) - mu) / std_error`. The second item represent the degrees of freedom used in calculating the p-value, which is equal to `length(x) -` `. The third item returns the type of alternative test the user specified. The fourth item is the p-value of null hypothesis test.

The first t-test I will demonstrate is:
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &\neq 60.
  \end{align}
This is an example of a two-sided t-test, with `mu` being equal to 60. The null hypothesis is that `mu` equals 60, and the alternative hypothesis is that mu is not equal to 60. Therefore we set up a `my_t.test` function that looks like this:
```{r}
my_t.test(x = my_gapminder$lifeExp, alternative = "two.sided", mu = 60)
```
This gives us a p-value of `r my_t.test(x = my_gapminder$lifeExp, alternative = "two.sided", mu = 60)[4]`. This is greater than the cut-off of 0.05, so we do not reject the null hypothesis.


The second t-test I will demonstrate is:
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &< 60.
  \end{align}
This is an example of a t-test where the alternative is less than `mu`, with `mu` being equal to 60. The null hypothesis is that `mu` equals 60, and the alternative hypothesis is that mu is less than 60. Therefore we set up a `my_t.test` function that looks like this:
```{r}
my_t.test(x = my_gapminder$lifeExp, alternative = "less", mu = 60)
```
This gives us a p-value of `r my_t.test(x = my_gapminder$lifeExp, alternative = "less", mu = 60)[4]`. This is less than the cut-off of 0.05, so we reject the null hypothesis.

The third t-test I will demonstrate is:
  \begin{align}
  H_0: \mu &= 60,\\
  H_a: \mu &> 60.
  \end{align}
This is an example of a t-test where the alternative is greater than `mu`, with `mu` being equal to 60. The null hypothesis is that `mu` equals 60, and the alternative hypothesis is that mu is greater than 60. Therefore we set up a `my_t.test` function that looks like this:
```{r}
my_t.test(x = my_gapminder$lifeExp, alternative = "greater", mu = 60)
```
This gives us a p-value of `r my_t.test(x = my_gapminder$lifeExp, alternative = "greater", mu = 60)[4]`. This is greater than the cut-off of 0.05, so we do not reject the null hypothesis.

## Tutorial for `my_lm`
This function produces a table with rows for each coefficient (beta), and columns for the estimates of all the coefficients, their standard error, the t value calculated by dividing the coefficient by the standard error, and `Pr(>|t|)`, which represents the probablity of the null hypothesis being true. The null hypothesis is that the coefficient is equal to zero.

An example regression, using `lifeExp` as the response variable, and `gdpPercap` and `continent` as explanatory variables:
```{r, warning=FALSE}
my_lm(lifeExp ~ gdpPercap + continent, my_gapminder)
```

The `gdpPercap` coefficient respresents the significance of `gdpPercap` as a covariate. A value of 0.0004453 indicates that the response differs by that amount for each increase of one unit of gdpPercap. The low value of the coefficient is probably because `gdpPercap` generally has values of several thousand, while continent is a four level factor.

The hypothesis test uses a t-value of 18.96606, which is calculated by dividing the estimate by the standard error. The hypothesis test is as follows:
  \begin{align}
  H_0: \beta &= 0,\\
  H_a: \beta &\neq 0.
  \end{align}
As we can see from `Pr(>|t|)` being zero, there is no probability of the null hypothesis being true, since there is zero probability of observing an absolute t value greater than the observed t value. Therefore, we reject the null hypothesis of the coefficient being equal to zero. 

Now we are going to compare the actual vs. fitted values:
```{r, warning=FALSE}
#Creates data frame of actual and fitted values
mod_fits <- fitted(lm(lifeExp ~ gdpPercap + continent, my_gapminder))
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = mod_fits)
#Plots actual vs. fitted graph
ggplot(my_df, aes(x = fitted, y = actual)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) + 
  theme_bw(base_size = 10) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))
```

As we can see in the graph, the model fit is relatively accurate, however, many of the actual values are incorrectly fitted into certain clusters. However, with higher values, the trend is very accurate, except for the few outliers the fitted model throws out.

## Tutorial for `my_knn_cv`
This function predicts the output class of a given dataset, using k-nearest-neighbor cross-validation. It also returns the cross-validation misclassification error, which is the proportion of output classes that were misclassified. What cross-validation does is split the data into `k_cv` folds, use all but one fold as the training data, fit the model, and use the unused fold as test data to make predictions. It then repeats this process for each fold until each fold has been used as the test data. Cross validation is useful because it allows the whole data to be used as training data, while still being able to calculate testing error. It only requires one dataset. 

This is an example using the `my_penguins` dataset, predicting output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. We will use 5 folds (`k_cv = 5`), and iterate for every value of `k_nn` between 1 and 10. We need to be sure to remove all na values and unnecessary covariates too.

```{r}
#removes observations with na values
train <- na.omit(my_penguins)
#removes unnecessary columns from dataset
train$island <- NULL
train$sex <- NULL
train$year <- NULL
cl <- train$species
#creates vectors to store training and cv misclassification errors
train_err <- rep(NA, 10)
cv_misclass_err <- rep(NA, 10)
#tests function, iterating with k_nn values from 1 to 10
for (i in 1:10) {
  knn_i <- my_knn_cv(train, cl, i, 5)
  cv_misclass_err[i] <- knn_i$cv_err
  #calculates training error
  train_err[i] <- mean((as.numeric(knn_i$class) - as.numeric(cl))^2)
}
my_df <- data.frame("k_nn" = c(1:10),
          "CV Misclassification Error" = cv_misclass_err,
          "Training Error" = train_err)
kable_styling(kable(my_df))
```
Based on the CV Misclassification error, I would chooose a model with `k_nn == 1`. I would do the same based on training error. Therefore I would most likely choose that model in practice because it has the lowest training and testing error.

```{r, eval=FALSE}
library(tidyr)
data(my_penguins) 
my_penguins <- drop_na(my_penguins)
my_rf_cv(2)
```

